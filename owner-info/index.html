<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
    <meta charset="utf-8" />
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" type="text/css" href="./fonts/stylesheet.css" charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="./css/fontawesome.min.css" charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="./css/brands.min.css" charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="./css/regular.min.css" charset="utf-8" />
    <link rel="stylesheet" type="text/css" href="./css/style.css" media="all" />
    <title>Qianyue He</title>
</head>

<body>
    <div class="mainbody">
        <div class="header-content">
            <h1><a href="/index.php">Qianyue He</a></h1>
        </div>
        <div class="line"></div>
        <div class="content">

            <div class="about">
                <a href="https://enigmatisms.github.io/"><img class="avatar" src="images/avatar.jpg"></a>

                <h1><i class="far fa-portal-exit"></i> About me</h1>

                <p>Hi, I'm Qianyue He! I am a graphics and HPC enthusiast, and I really enjoy coding just for fun. Some people might ask me, "Why did you start this project?" or "What is the commercial potential of your repo?" but I don't want to bother answering them.
                    Why does everything have to bear some specific meaning? I think those things are cool, and if I can nail them, that will be awesomeâ€”simple as that. The knowledge I gain is just an extra bonus for me.
                </p>

                <p style="margin-top: 10px;">
                    I am a master's student (will graduate in 2025) in AI in <a style="color: #9b0098;" href="https://www.sigs.tsinghua.edu.cn/en/">Tsinghua SIGS</a>, where I 
                    did some minor stuff for time-of-flight rendering. I majored in automation and earned my Bachelor of Engineering in
                    <a href="http://en.xjtu.edu.cn/">XJTU</a>, and I believe XJTU is where I truly grew, technically, thanks to the help and guidance of several 
                    good friends and mentors. My life as an undergrad can be described as, I think, very fulfilling. I joined the <a style="color: #000088;" href="https://www.robomaster.com/zh-CN">RoboMaster</a> team of our school for two years (2020-2021), in the vision group and took part in the group leader for one season (2021), where I built an awesome auto-targeting system with my friends. Later I was in the <a style="color: #1377fa;" href="https://www.sigs.tsinghua.edu.cn/en/">IAIR</a>, working on CV and SLAM for two years.

                    I like working on my own open-source projects in my free time (sounds nerdy, well I actually am). My recent focus is rendering, especially, ray tracing frameworks. I've tweaked many open-source renderers like <a href="https://www.mitsuba-renderer.org/">Mitsuba</a>, <a href="https://github.com/mmp/pbrt-v3">PBRT</a> and <a href="https://benedikt-bitterli.me/tungsten.html">Tungsten</a>, and I've written two renderers: <a style="color: #af0000;" href="https://github.com/Enigmatisms/AdaPT">AdaPT</a> and <a style="color: #00a862;" href="https://github.com/Enigmatisms/cuda-pt">cuda-pt</a> for fun. Also, I work as a pro-tem moderator for <a href="https://computergraphics.stackexchange.com/">computer graphics StackExchange</a> (2024~now). Being working hard for the community, I guess.
                </p>

                <p style="margin-top: 10px;">
                    My interests are now shifting toward something more fundamental. I plan to work in the fields of AI infrastructure and compilersâ€”they're so cool, aren't they? Wish me luck in my exploration. I assume I won't have the same state of mind when I am 30, so, gotta do something interesting when you are young.
                </p>

                <p style="margin-top: 10px;">
                    Got married to my junior & senior high classmate in my 24! She is such a heavenly girl.
                </p>

                <p class="resourcelist" style="margin-top: 10px;">
                    <a href="984041003@qq.com"><i class="far fa-envelope"></i> Email</a>
                    <a href="https://github.com/Enigmatisms"><i class="fab fa-github"></i> GitHub</a>
                    <a href="https://enigmatisms.github.io/"><i class="far fa-wifi"></i> Blog</a>
                </p>
            </div>

            <h1 id="sec-publications"><i class="far fa-user-graduate"></i> Education</h1>

            <section class="education">
                <div class="degree left">
                  <div class="school">
                    <img src="./images/tsinghua.jpg" alt="Tsinghua" class="logo">
                    <span class="school-name">Tsinghua University, SIGS</span>
                  </div>
                  <div class="details">
                    <span class="degree-time">2022.9-2025.6 (EST)</span>
                    <span class="major">Master of Engineering in Artificial Intelligence</span>
                  </div>
                </div>
                <div class="degree right">
                  <div class="school">
                    <img src="./images/xjtu.jpg" alt="undergrad" class="logo">
                    <span class="school-name">Xi'an Jiaotong University</span>
                  </div>
                  <div class="details">
                    <span class="degree-time">2018.9-2022.6</span>
                    <span class="major">BEng. in Automation</span>
                  </div>
                </div>
            </section>
            <br/>

            <h1 id="sec-publications"><i class="far fa-podium-star"></i> Publications</h1>

            <div id="portfolio" class="portfolio-item"
                onmouseover="document.getElementById('he24darts-video').play()"
                onmouseout="document.getElementById('he24darts-video').pause()">
                <div class="portfolio-image">
                    <a href="https://darts-paper.github.io/DARTS-proj-page/">
                        <video muted loop preload="auto" poster="images/he24darts/darts.jpg" id="he24darts-video">
                            <source src="images/he24darts/darts.webm" type="video/webm; codecs=vp9">
                        </video>
                    </a>
                </div>
                <div class="portfolio-text">
                    <h2><a href="https://darts-paper.github.io/DARTS-proj-page/">ðŸŽ¯DARTS: Diffusion Approximated Residual Time Sampling for Time-of-flight Rendering in Homogeneous Scattering Media</a></h2>
                    <p class="authorlist"><a style="white-space: nowrap; color: #a680ff"
                            href="https://enigmatisms.github.io/owner_info/">Qianyue He</a>, <a style="white-space: nowrap;"
                            href="https://dongyu-du.github.io/">Dongyu Du</a>, Haitian Jiang,
                        <a style="white-space: nowrap;" href="https://scholar.google.com.hk/citations?user=FTikW50AAAAJ&hl=zh-CN&oi=sra">Xin Jin</a>
                    <p class="venue"><a href="https://dl.acm.org/doi/10.1145/3687930">ACM Transactions on Graphics
                            (<strong>SIGGRAPH Asia</strong>)</a>, Dec 2024</p>
                    <p class="venue"><i class="far fa-award"></i> <strong>SIGGRAPH Asia</strong> Longest Paper Name Award (2nd place, well this is made-up)</p>
                    <p class="resourcelist"><span style="white-space: nowrap;"><a
                                href="https://dl.acm.org/doi/pdf/10.1145/3687930"><i
                                    class="far fa-file-alt"></i> Paper</a></span> <a
                                href="https://darts-paper.github.io/DARTS-proj-page/static/pdfs/supp-note-final.pdf"><i
                                    class="far fa-file-alt"></i> Supplementary</a></span> <span
                            style="white-space: nowrap;"><a
                                href="https://www.youtube.com/watch?v=aOQPA7ccOtY"><i
                                    class="far fa-video"></i> Video</a></span> <span
                            style="white-space: nowrap;"><a
                                href="https://darts-paper.github.io/DARTS-proj-page/"><i
                                    class="far fa-external-link"></i> Project Page</a></span> <span
                            style="white-space: nowrap;"><a
                                href="./data/ref.bib"><i
                                    class="far fa-quote-right"></i> BibTeX</a></span></p>
                </div>
                <div style="clear: both"></div>
            </div>

            <h1 id="sec-publications"><i class="far fa-university"></i> Thesis</h1>

            <div id="portfolio" class="portfolio-item">
                <div class="portfolio-image"><video muted loop preload="auto"
                            poster="images/SLAM.jpg" id="bch-video">
                        </video></div>
                <div class="portfolio-text">
                    <h2>Chain-representation-based real time high precision single line LiDAR SLAM system</h2>
                    <p class="venue"><b>Bachelor thesis (61 pages) in XJTU</b>
                        <p style="text-align: justify;">In this thesis, a novel map representation named Chain Representation is proposed. This representation is sort of a mixture that has both the advantages of point cloud and occupancy map. The proposed SLAM system is compared against the SoTA method (in 2022): Google's cartographer, SLAM toolbox and the classic GMapping. The proposed method demonstrates faster mapping, higher accuracy and lower memory overhead. The code is not currently open-sourced.
                        </p>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>


            
            <h1 id="sec-projects"><i class="far fa-hat-wizard"></i> Selected Personal Projects</h1>

            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('adapt-video').play()"
                onmouseout="document.getElementById('adapt-video').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/AdaPT"><video muted loop preload="auto"
                            poster="images/adapt.jpg" id="adapt-video">
                            <source src="images/adapt.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/AdaPT">The AdaPT renderer</a></h2>
                    <p class="venue"><b>2023.1~2024.11</b>
                        <p style="text-align: justify;">A physically based renderer built in Taichi lang (python-like, JIT). This offline renderer supports steady state and transient state rendering, with undirectional, volumetric and bidirectional path tracing implementation. Textures, heterogeneous RGB density volumes and different kinds of emitters are supported. Customized Mitsuba-like XML scene parsing, with stackless SAH-LBVH ray-primitive acceleration.
                        </p>
                    </p>
                    <p class="resourcelist">
                        <span style="white-space: nowrap;"><a href="https://enigmatisms.github.io/2023/02/09/AdaPT-Monte-Carlo-Path-Tracer-I/"><i
                                    class="far fa-rocket-launch"></i> Blogs</a></span> <span
                            style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/AdaPT"><i
                                    class="far fa-code"></i> Code</a></span>
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>

            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('cupt-video').play()"
                onmouseout="document.getElementById('cupt-video').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/cuda-pt"><video muted loop preload="auto"
                            poster="images/cuda-pt.jpg" id="cupt-video">
                            <source src="images/cuda-pt.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/cuda-pt">CUDA-PT</a></h2>
                    <p class="venue"><b>2024.4~2024.12</b>
                        <p style="text-align: justify;">A repo that teaches you how to write a CUDA software ray tracing renderer from scratch: a renderer with Analysis-Driven Optimization. The core logic and basic building blocks are all written from scratch in CUDA, and they are carefully profiled and optimized to have better performance. Definitely not a naive porting of CPU implementation. Megakernel path tracing, light tracing and wavefront path tracing are supported. 
                        </p>
                    </p>
                    <p class="resourcelist">
                       <span style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/cuda-pt"><i
                                    class="far fa-code"></i> Code</a></span>
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>
            
            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('lidarsim2d-vid').play()"
                onmouseout="document.getElementById('lidarsim2d-vid').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/LSMv2"><video muted loop preload="auto"
                            poster="images/lidarsim2d.jpg" id="lidarsim2d-vid">
                            <source src="images/lidarsim2d.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/LSMv2">LiDARSim2D & LSMv2</a></h2>
                    <p class="venue"><b>2021.8~2022.8</b>
                        <p style="text-align: justify;">Two different versions of 2D LiDAR simulators. The first version is written in C++, with OpenCV and ROS Rviz visualization. Supports exporting ROS bags of scan data, IMU data and etc.. The second version is written in Rust and CUDA, with nannou as its front-end visualization and GUI and CUDA based accelerated ray-intersection code as simulation backend. Both support map editting and customization. 
                        </p>
                    </p>
                    <p class="resourcelist">
                        <span style="white-space: nowrap;"><a href="https://enigmatisms.github.io/2021/08/28/2D-LiDAR-Simulator-Release-Notice/"><i
                                    class="far fa-rocket-launch"></i> Blogs</a></span> <span
                            style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/LiDARSim2D"><i
                                    class="far fa-code"></i> Code V1</a></span> <span
                            style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/LSMv2"><i
                                    class="far fa-code"></i> Code V2</a></span>
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>

            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('ethians-vid').play()"
                onmouseout="document.getElementById('ethians-vid').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/Ethians-Alpha-1.1"><video muted loop preload="auto"
                            poster="images/ethians.jpg" id="ethians-vid">
                            <source src="images/ethians.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/Ethians-Alpha-1.1">The Ethians (Rougelike Game)</a></h2>
                    <p class="venue"><b>2019.1~2019.8</b>
                        <p style="text-align: justify;">The first big project of mine, written during the first year of my undergrad. I love this rougelike game very much: I extracted the assets from an old mobile rougelike game called <i>Dweller</i> and rewrite all the game play in PyGame. The GUI and some of the art assets are designed and drawn on my own. I poured much time and effort into this, and till today, I can still say that it beats most of the undergrad Graduation Designs in our country (since most of them are so shitty).
                        </p>
                    </p>
                    <p class="resourcelist">
                        <span style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/Ethians-Alpha-1.1"><i
                                    class="far fa-code"></i> Code</a></span> 
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>

            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('axis6-video').play()"
                onmouseout="document.getElementById('axis6-video').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/Axis6"><video muted loop preload="auto"
                            poster="images/axis6.jpg" id="axis6-video">
                            <source src="images/axis6.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/Axis6">6-axis Robot Simulator</a></h2>
                    <p class="venue"><b>2022.10</b>
                        <p style="text-align: justify;"> 6-axis robot simulation in rviz and Gazebo, control implemented with ROS. Simple hand-deduced D-H forward-inverse kinematics implemented in Eigen. Simulated and visualized in RViz and Gazebo ROS implementation (melodic), with key control. 
                        </p>
                    </p>
                    <p class="resourcelist">
                        <span style="white-space: nowrap;"><a href="https://enigmatisms.github.io/2021/10/31/%E4%BD%8D%E5%A7%BF%E5%8F%98%E6%8D%A2%E4%B8%8E%E5%85%AD%E8%BD%B4%E4%BB%BF%E7%9C%9F/"><i
                                    class="far fa-rocket-launch"></i> Blogs</a></span> <span
                            style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/Axis6"><i
                                    class="far fa-code"></i> Code </a></span>
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>

            <div id="portfolio" class="portfolio-item" 
                onmouseover="document.getElementById('giga-video').play()"
                onmouseout="document.getElementById('giga-video').pause()">
                <div class="portfolio-image"><a href="https://github.com/Enigmatisms/nerfstudio-giga"><video muted loop preload="auto"
                            poster="images/giga.jpg" id="giga-video">
                            <source src="images/giga.webm" type="video/webm; codecs=vp9">
                        </video></a></div>
                <div class="portfolio-text">
                    <h2><a href="https://github.com/Enigmatisms/nerfstudio-giga">GigaMVS Novel View Synthesis Competition Solution</a></h2>
                    <p class="venue"><b>2023.4~2023.6</b>
                        <p style="text-align: justify;">GAIIC 2023: giga-pixel novel view synthesis competition 4th place solution. Based on nerfstudio. The other two contributors are <a href="https://github.com/Dinngger">Dinnger</a> and <a href="https://github.com/longzw001116">funnymudpeer</a>. The ranks can be found in giga-vision's leaderboard for rendering challenge. The competition focuses on the novel-view synthesis problem under vast scenes and huge input images. We reproduced many SoTA works in this repo to obtain higher PSNR.
                        </p>
                    </p>
                    <p class="resourcelist">
                        <span style="white-space: nowrap;"><a href="https://gigavision.cn/track/track/?nav=Sparse%20rendering&type=nav"><i
                                    class="far fa-rocket-launch"></i> GigaMVS</a></span> <span
                            style="white-space: nowrap;"><a href="https://github.com/Enigmatisms/nerfstudio-giga"><i
                                    class="far fa-code"></i> Code</a></span> <span
                            style="white-space: nowrap;"><a href="https://docs.google.com/presentation/d/16zcceAIQLTjLUsLQfgxJlnt-Mq_6Hl5w/edit?rtpof=true&sd=true&pli=1"><i
                                    class="far fa-clipboard"></i> Slides</a></span>
                        </span>
                    </p>
                </div>
                <div style="clear: both"></div>
            </div>
        </div>

        <h1 id="sec-publications"><i class="far fa-person-dolly"></i> Experience</h1>

        <p><b>2024.5 - 2024.11 Tencent LightSpeed Studios</b> | Graphics research: Dynamic voxelization of foliage meshes and volumetric representation that enables multiple scattering.
        </p>

        <p><b>2021.6 - 2021.8 XJSY Research Institute of AI </b> | SLAM R&D: Chain representation of 2D LiDAR maps. The algorithm development of the SLAM system.
        </p>
        <br/>

        <h1 id="sec-publications"><i class="far fa-trophy-alt"></i> Selected Awards</h1>
        <ul>
            <li>(2024) Second-class Scholarship of Tsinghua University</li>
            <li>(2022) XJTU Outstanding graduates</li>
            <li>(2021) Second-class Scholarship of XJTU</li>
            <li>(2020) National Scholarship</li>
            <li>(2019) National Scholarship</li>
            <li>3 years of outstanding students (2019, 2020, 2021)</li>
          </ul>

        <div class="footer-content">
            <p style="clear: both"><strong>114,514</strong> views | query took <strong>0.1919810</strong> years</p>
            <div class="thinline"></div>
            <p>&copy; 2023-2025 Qianyue He <i class="far fa-portal-enter"></i></p>
        </div>
    </div>
</body>

</html>